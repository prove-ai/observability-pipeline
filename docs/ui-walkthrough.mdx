## Dashboard Overview 
The Prove AI dashboard is an observability platform designed for containerized generative AI workloads. The platform provides monitoring, evaluation, and compliance capabilities for LLM inference services, RAG pipelines, and embedding systems.

The following components are currently available in the dashboard:
- Dashboard: Customizable metrics visualization and incident management 
- Configuration: Prometheus integration and notification setup

**Note:** You will see other components which are not accessible by default; contact the Prove AI team to learn how to enhance your compliance monitoring.

## Signing In/Making an Account
The application is accessible at https://proveai-integration.proveai.com/

You may log in using email and password credentials. To set up a new account, you must use a professional email address (personal email addresses won't work, at present).

## Dashboard 
The Dashboard provides a customizable interface for monitoring infrastructure and model performance metrics. Users can add individual metrics and create visualizations to track system health and performance trends.

The main elements are described below.

- Time Range Selector (top left): Dropdown menu for selecting the data time window:
    - Last 24h
    - Last 7d
    - Last 30d
- Refresh Data (top right): Manually refresh dashboard data to display the latest metrics.
- Create Incident (top right): Open the incident creation panel for documenting issues or outages.
- Add Metric panels: Empty metric cards for displaying individual metric values or simple visualizations. These provide at-a-glance monitoring for key performance indicators.
- Add Chart panels: Larger panels for creating complex visualizations including time-series graphs, histograms, and other chart types for analyzing metric trends over time.

### Adding Metrics
To add a metric to the dashboard:

- Click the + Add Metric button
- Use the search function to locate the desired metric
- Select the metric from the results list

The platform supports three primary metric types:
- Counter: Tracks cumulative values that only increase over time (e.g., total requests, token counts, successfully processed requests)
- Gauge: Represents current state values that can increase or decrease (e.g., queue depth, active connections, KV-cache usage percentage)
- Histogram: Provides distribution data for debugging performance issues. Histograms track percentile distributions (p50, p95, p99), which are more operationally relevant than averages for understanding LLM system behavior, particularly for latency and token generation metrics.

A full list of available metrics can be found in Appendix A

### Creating an Incident
If you click 'create incident,' you'll be shown a panel that allows you to document and track issues or outages. The panel includes:

- Integration selector: Shows which system will receive the incident (in this case, GitHub). You can click "Manage integrations" to configure where incidents are sent.
- Title field: A text input for a descriptive incident name, with a helpful example placeholder: "p95 latency spiking on vLLM."
- Date and time pickers: Set to the current date/time (e.g., Jan 21, 2026, 01:09 PM), allowing you to specify when the incident occurred or was detected.
- Description field: A larger text area for detailed information including what happened, the scope of the issue, suspected causes, steps already taken, and links to relevant traces or logs for investigation.
- Create Incident button: Submits the incident to your configured integration (like creating a GitHub issue).
- Cancel button: Closes the panel without creating an incident.

This is a streamlined way to create incident reports directly from your observability dashboard, automatically linking them to your existing workflow tools.

## Configuration 
The Configuration page establishes connections between Prove AI and external monitoring and incident management systems.

### Environment Setup
Here, you can configure monitoring environment and data source connections.

**Prometheus Connection** 

Status badge: Displays connection state (e.g., "Connected")
URL field: Enter Prometheus endpoint (e.g., https://obs-dev.proveai.com:9090). **If you have your own Prometheus instance running, you would put its URL here.**

Use Basic Auth toggle: Enable if Prometheus requires authentication
- When enabled, displays:
    - Username field
    - Password field

Use Proxy toggle: Enable if Prometheus requires proxy connection
- When enabled, displays:
    - A radio button with which you can select the protocol (HTTP, HTTPS)
    - A field to enter the proxy URL (e.g., `proxy.company.com`)
    - An additional toggle you can enable if your proxy requires authentication  
        - If you enable proxy authentication, the interface will display:
            - The proxy username field
            - The proxy password field

**Connection status panel**
- Click `Test connection` to run an assessment of your connection
- Real-time log displaying:
    - Timestamp for each operation
    - Test progress messages
    - HTTP requests and responses
    - Status confirmation

Connection result message: Displays outcome at bottom of panel (e.g., `Status: healthy`)

Click `Save` to store configuration settings

### Getting Started
Sidebar panel (right):
- Documents section: Link to View Setup Github Repo containing observability pipeline documentation, where you can find instructions for installing OTel/OpenLLMetry to emit AI metrics

### Integrations & Notifications
Here, you can set up infrastructure to create tickets and send messages with the context your engineers need.

GitHub Integration:
- Status badge: Displays connection state (e.g., "Connected")
- Settings button: Configure GitHub integration details
- Disable button (red): Disconnect the integration

Jira Integration:
- Status badge: Displays connection state (e.g., "Not connected")
- Settings button: Configure Jira integration details
- Connect button (white): Establish the integration
- Disable button (red, if connected): Disconnect the integration

The Configuration page centralizes observability infrastructure connections and incident management integrations, enabling automated ticket creation and notification workflows.






Appendix A - Available Metrics
List of metrics:
- vllm:prefix_cache_hits_total (COUNTER) - Prefix cache hits, in terms of number of cached tokens.
- vllm:external_prefix_cache_hits_created - External prefix cache hits from KV connector cross-instance cache sharing, in terms of num[ber]
- vllm:request_prefill_time_seconds_created (GAUGE) - Histogram of time spent in PREFILL phase for request.
- vllm:prefix_cache_hits_created (GAUGE) - Prefix cache hits, in terms of number of cached tokens.
- vllm:request_success_created (GAUGE) - Count of successfully processed requests.
- vllm:request_max_num_generation_tokens_created (GAUGE) - Histogram of maximum number of requested generation tokens.
- vllm:request_queue_time_seconds_created (GAUGE) - Histogram of time spent in WAITING phase for request.
- vllm:e2e_request_latency_seconds (HISTOGRAM) - Histogram of e2e request latency in seconds.
- vllm:request_decode_time_seconds_created (GAUGE) - Histogram of time spent in DECODE phase for request.
- vllm:engine_sleep_state - Engine sleep state; awake = 0 means engine is sleeping; awake = 1 means engine is awake;
- vllm:request_prompt_tokens (HISTOGRAM) - Number of prefill tokens processed.
- vllm:inter_token_latency_seconds_created (GAUGE) - Histogram of inter-token latency in seconds.
- vllm:request_inference_time_seconds (HISTOGRAM) - Histogram of time spent in RUNNING phase for request.
- vllm:num_preemptions_total (COUNTER) - Cumulative number of preemption from the engine.
- vllm:time_to_first_token_seconds_created (GAUGE) - Histogram of time to first token in seconds.
- vllm:request_time_per_output_token_seconds (HISTOGRAM) - Histogram of time_per_output_token_seconds per request.
- vllm:prefix_cache_queries_created (GAUGE) - Prefix cache queries, in terms of number of queried tokens.
- vllm:request_prompt_tokens_created (GAUGE) - Number of prefill tokens processed.
- vllm:request_queue_time_seconds (HISTOGRAM) - Histogram of time spent in WAITING phase for request.
- vllm:request_inference_time_seconds_created (GAUGE) - Histogram of time spent in RUNNING phase for request.
- vllm:mm_cache_hits_created (GAUGE) - Multi-modal cache hits, in terms of number of cached items.
- vllm:generation_tokens_total (COUNTER) - Number of generation tokens processed.
- vllm:request_success_total (COUNTER) - Count of successfully processed requests.
- vllm:request_generation_tokens_created (GAUGE) - Number of generation tokens processed.
- vllm:inter_token_latency_seconds (HISTOGRAM) - Histogram of inter-token latency in seconds.
- vllm:cache_config_info (GAUGE) - Information of the LLMEngine CacheConfig
- vllm:iteration_tokens_total (HISTOGRAM) - Histogram of number of tokens per engine_step.
- vllm:request_params_n_created (GAUGE) - Histogram of the n request parameter.
- vllm:mm_cache_hits_total (COUNTER) - Multi-modal cache hits, in terms of number of cached items.
- vllm:request_params_max_tokens (HISTOGRAM) - Histogram of the max_tokens request parameter.
- vllm:request_prefill_time_seconds (HISTOGRAM) - Histogram of time spent in PREFILL phase for request.
- vllm:num_requests_waiting (GAUGE) - Number of requests waiting to be processed.
- vllm:kv_cache_usage_perc (GAUGE) - KV-cache usage. 1 means 100 percent usage.
- vllm:mm_cache_queries_created (GAUGE) - Multi-modal cache queries, in terms of number of queried items.
- vllm:generation_tokens_created (GAUGE) - Number of generation tokens processed.
- vllm:iteration_tokens_total_created (GAUGE) - Histogram of number of tokens per engine_step.
- vllm:request_max_num_generation_tokens (HISTOGRAM) - Histogram of maximum number of requested generation tokens.
- vllm:prefix_cache_queries_total (COUNTER) - Prefix cache queries, in terms of number of queried tokens.
- vllm:request_generation_tokens (HISTOGRAM) - Number of generation tokens processed.
- vllm:num_requests_running (GAUGE) - Number of requests in model execution batches.
- vllm:external_prefix_cache_queries_total - External prefix cache queries from KV connector cross-instance cache sharing, in terms of n[umber]
- vllm:request_params_max_tokens_created (GAUGE) - Histogram of the max_tokens request parameter.
- vllm:mm_cache_queries_total (COUNTER) - Multi-modal cache queries, in terms of number of queried items.
- vllm:request_params_n (HISTOGRAM) - Histogram of the n request parameter.
- vllm:num_preemptions_created (GAUGE) - Cumulative number of preemption from the engine.
- vllm:prompt_tokens_total (COUNTER) - Number of prefill tokens processed.
- vllm:prompt_tokens_created (GAUGE) - Number of prefill tokens processed.
- vllm:external_prefix_cache_queries_created - External prefix cache queries from KV connector cross-instance cache sharing, in terms of n[umber]
- vllm:external_prefix_cache_hits_total - External prefix cache hits from KV connector cross-instance cache sharing, in terms of nu[mber]
- vllm:time_to_first_token_seconds (HISTOGRAM) - Histogram of time to first token in seconds.
- vllm:request_time_per_output_token_seconds_created (GAUGE) - Histogram of time_per_output_token_seconds per request.
- vllm:e2e_request_latency_seconds_created (GAUGE) - Histogram of e2e request latency in seconds.
- vllm:request_decode_time_seconds (HISTOGRAM) - Histogram of time spent in DECODE phase for request.
- vllm:request_time_per_output_token_seconds_created (GAUGE)
- vllm:request_inference_time_seconds (HISTOGRAM) - Histogram of time spent in RUNNING phase for request.
- vllm:num_preemptions_total (COUNTER) - Cumulative number of preemption from the engine.
- vllm:generation_tokens_total (COUNTER) - Number of generation tokens processed.
- vllm:request_max_num_generation_tokens (HISTOGRAM) - Histogram of maximum number of requested generation tokens.
- vllm:inter_token_latency_seconds_created (GAUGE) - Histogram of inter-token latency in seconds.
- vllm:prefix_cache_queries_total (COUNTER) - Prefix cache queries, in terms of number of queried tokens.
- vllm:request_generation_tokens_created (GAUGE) - Number of generation tokens processed.
- vllm:request_prompt_tokens_created (GAUGE) - Number of prefill tokens processed.
- vllm:iteration_tokens_total (HISTOGRAM) - Histogram of number of tokens per engine_step.
- vllm:request_prefill_time_seconds (HISTOGRAM) - Histogram of time spent in PREFILL phase for request.
- vllm:mm_cache_queries_created (GAUGE) - Multi-modal cache queries, in terms of number of queried items.
- vllm:generation_tokens_created (GAUGE) - Number of generation tokens processed.
- vllm:request_success_total (COUNTER) - Count of successfully processed requests.
- vllm:iteration_tokens_total_created (GAUGE) - Histogram of number of tokens per engine_step.
- vllm:e2e_request_latency_seconds (HISTOGRAM) - Histogram of e2e request latency in seconds.
- vllm:mm_cache_hits_created (GAUGE) - Multi-modal cache hits, in terms of number of cached items.
- vllm:prompt_tokens_created (GAUGE) - Number of prefill tokens processed.
- vllm:request_params_max_tokens_created (GAUGE) - Histogram of the max_tokens request parameter.
- vllm:request_prefill_time_seconds_created (GAUGE) - Histogram of time spent in PREFILL phase for request.
- vllm:request_generation_tokens (HISTOGRAM) - Number of generation tokens processed.
- vllm:request_decode_time_seconds (HISTOGRAM) - Histogram of time spent in DECODE phase for request.
- vllm:kv_cache_usage_perc (GAUGE) - KV-cache usage. 1 means 100 percent usage.
- vllm:prefix_cache_queries_created (GAUGE) - Prefix cache queries, in terms of number of queried tokens.
- vllm:external_prefix_cache_queries_created - External prefix cache queries from KV connector cross-instance cache sharing, in terms of n[umber]
- vllm:external_prefix_cache_hits_created - External prefix cache hits from KV connector cross-instance cache sharing, in terms of num[ber]
- vllm:prompt_tokens_total (COUNTER) - Number of prefill tokens processed.
- vllm:request_queue_time_seconds (HISTOGRAM) - Histogram of time spent in WAITING phase for request.
- vllm:external_prefix_cache_queries_total - External prefix cache queries from KV connector cross-instance cache sharing, in terms of n[umber]
- vllm:external_prefix_cache_hits_total - External prefix cache hits from KV connector cross-instance cache sharing, in terms of num[ber]
- vllm:request_success_created (GAUGE) - Count of successfully processed requests.
- vllm:request_inference_time_seconds_created (GAUGE) - Histogram of time spent in RUNNING phase for request.
- vllm:num_requests_waiting (GAUGE) - Number of requests waiting to be processed.
- vllm:mm_cache_hits_total (COUNTER) - Multi-modal cache hits, in terms of number of cached items.
- vllm:mm_cache_queries_total (COUNTER) - Multi-modal cache queries, in terms of number of queried items.
- vllm:request_params_n (HISTOGRAM) - Histogram of the n request parameter.
- vllm:num_requests_running (GAUGE) - Number of requests in model execution batches.
- vllm:engine_sleep_state - Engine sleep state; awake = 0 means engine is sleeping; awake = 1 means engine is awake;
- vllm:request_params_max_tokens (HISTOGRAM) - Histogram of the max_tokens request parameter.
- vllm:request_decode_time_seconds_created (GAUGE) - Histogram of time spent in DECODE phase for request.
- vllm:request_params_n_created (GAUGE) - Histogram of the n request parameter.
- vllm:time_to_first_token_seconds (HISTOGRAM) - Histogram of time to first token in seconds.
- vllm:request_time_per_output_token_seconds (HISTOGRAM) - Histogram of time_per_output_token_seconds per request.
- vllm:e2e_request_latency_seconds_created (GAUGE) - Histogram of e2e request latency in seconds.
- vllm:cache_config_info (GAUGE) - Information of the LLMEngine CacheConfig
- vllm:num_preemptions_created (GAUGE) - Cumulative number of preemption from the engine.
- vllm:request_max_num_generation_tokens_created (GAUGE) - Histogram of maximum number of requested generation tokens.
- vllm:prefix_cache_hits_total (COUNTER) - Prefix cache hits, in terms of number of cached tokens.
- vllm:prefix_cache_hits_created (GAUGE) - Prefix cache hits, in terms of number of cached tokens.
- vllm:time_to_first_token_seconds_created (GAUGE) - Histogram of time to first token in seconds.
- vllm:inter_token_latency_seconds (HISTOGRAM) - Histogram of inter-token latency in seconds.
- vllm:request_queue_time_seconds_created (GAUGE) - Histogram of time spent in WAITING phase for request.
